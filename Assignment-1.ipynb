{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7fc0000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb85b6",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3499e51",
   "metadata": {},
   "source": [
    "> Preprocessing Train/Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e0533491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tfile):\n",
    "    with open(tfile, 'r') as file:\n",
    "        lst = file.read()\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    text = re.sub(pattern, '', lst)\n",
    "    text = text.lower()\n",
    "    data = text.strip().split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438032c9",
   "metadata": {},
   "source": [
    "> Train the Model using Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "393d03f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data):\n",
    "    unigrams = defaultdict(float)\n",
    "    bigrams = defaultdict(float)\n",
    "    \n",
    "    for word in train_data:\n",
    "        unigrams[word] = unigrams.get(word, 0) + 1\n",
    "\n",
    "    for i in range(len(train_data) - 1):\n",
    "        bigram = (train_data[i], train_data[i + 1])\n",
    "        bigrams[bigram] = bigrams.get(bigram, 0) + 1\n",
    "        \n",
    "    return unigrams, bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c47a9",
   "metadata": {},
   "source": [
    "> Calculate Probabilities of Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7208e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_probabilities(unigrams, bigrams):\n",
    "    unigram_prob = defaultdict(float)\n",
    "    total_words = sum(unigrams.values())   \n",
    "    bigram_prob = defaultdict(float)\n",
    "\n",
    "    for word in unigrams:\n",
    "        unigram_prob[word] = unigrams.get(word, 0) / total_words\n",
    "        \n",
    "        \n",
    "    for bigram in bigrams:\n",
    "        prev_word,current_word = bigram\n",
    "        bigram_prob[bigram] = bigrams.get(bigram,0) / unigrams.get(prev_word, 0)\n",
    "\n",
    "    \n",
    "    return unigram_prob, bigram_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b0bbeaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8699\n",
      "79399\n"
     ]
    }
   ],
   "source": [
    "training_file = \"/Users/chaitradaddolu/Desktop/nlp/Assignment1-CXD210026-KXS210121/A1_DATASET/train.txt\"\n",
    "test_file = \"/Users/chaitradaddolu/Desktop/nlp/Assignment1-CXD210026-KXS210121/A1_DATASET/val.txt\"\n",
    "train_data = preprocess(training_file)\n",
    "test_data = preprocess(test_file)\n",
    "N = len(test_data)\n",
    "n = len(train_data)\n",
    "print(N)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "399136cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams = train_model(train_data)\n",
    "vocab_size = len(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4cf72713",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7983af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cd8ecc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_probablities_of_train_data, bigram_probablities_of_train_data = cal_probabilities(unigrams, bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0eda6259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for word in unigram_probablities_of_train_data:\n",
    "#     print(word, \"-\", unigram_probablities_of_train_data[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "be6a5392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bigram in bigram_probablities_of_train_data:\n",
    "#     print(bigram, \"-\", bigram_probablities_of_train_data[bigram])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58d7d9",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276234cb",
   "metadata": {},
   "source": [
    ">  Calculate Unigram total log probability using UNK method by creating Implicit vocabulary with size v and choosing top v words by frequency and replacing rest with UNK. Here size is choosen as 3076 where the bottom tokens have occurence less than frequency of 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4c7ff3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_log_prob_unk_method1(test_data, unigrams):\n",
    "    total_log_prob = 0\n",
    "    total_words = sum(unigrams.values())\n",
    "    unigram_probs = defaultdict(float)\n",
    "    \n",
    "    unigrams_sorted = sorted(unigrams, key=unigrams.get, reverse = True)\n",
    "    \n",
    "    for i in range(3076):\n",
    "        word = unigrams_sorted[i]\n",
    "        unigram_probs[word] = (unigrams.get(word, 0)) / total_words\n",
    "\n",
    "    unigram_probs['<UNK>'] = (len(unigrams)-3076) / total_words\n",
    "    \n",
    "\n",
    "    for word in test_data:\n",
    "        if word not in unigram_probs:\n",
    "            word = '<UNK>'\n",
    "        total_log_prob += math.log(unigram_probs[word])\n",
    "\n",
    "\n",
    "    return total_log_prob "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09baf75",
   "metadata": {},
   "source": [
    "> Calculate Unigram total log probability using UNK method as described above and do laplace smoothing on the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "254e77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_log_prob_unk_method1_laplace(test_data, unigrams):\n",
    "    total_log_prob = 0\n",
    "    total_words = sum(unigrams.values())\n",
    "    unigram_probs = defaultdict(float)\n",
    "    vocab_size_new = 3076\n",
    "    \n",
    "    unigrams_sorted = sorted(unigrams, key = unigrams.get, reverse = True)\n",
    "\n",
    "    for i in range(3076):\n",
    "        word = unigrams_sorted[i]\n",
    "        unigram_probs[word] = (unigrams.get(word, 0) + 1) / (total_words + vocab_size_new)\n",
    "\n",
    "    unigram_probs['<UNK>'] = ((len(unigrams) - vocab_size_new) + 1) / (total_words + vocab_size_new)\n",
    "    \n",
    "    for word in test_data:\n",
    "        if word not in unigram_probs:\n",
    "            word = '<UNK>'\n",
    "        total_log_prob += math.log(unigram_probs[word])\n",
    "        \n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d7a64",
   "metadata": {},
   "source": [
    "> Calculate Unigram total log probability using UNK method as described above and do add-k smoothing on the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "161b0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_log_prob_unk_method1_add_k(test_data, unigrams, k):\n",
    "    total_log_prob = 0\n",
    "    total_words = sum(unigrams.values())\n",
    "    unigram_probs = defaultdict(float)\n",
    "    vocab_size_new = 3076\n",
    "     \n",
    "    unigrams_sorted = sorted(unigrams, key = unigrams.get, reverse=True)\n",
    "\n",
    "    for i in range(3076):\n",
    "        word = unigrams_sorted[i]\n",
    "        unigram_probs[word] = (unigrams.get(word, 0) + k) / (total_words + (k * vocab_size_new))\n",
    "\n",
    "    unigram_probs['<UNK>'] = ((len(unigrams)-vocab_size_new) + k) / (total_words + ( k * vocab_size_new))\n",
    "    \n",
    "    for word in test_data:\n",
    "        if word not in unigram_probs:\n",
    "            word = '<UNK>'\n",
    "        total_log_prob += math.log(unigram_probs[word])\n",
    "        \n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ee7ba",
   "metadata": {},
   "source": [
    "> Calculate Unigram total log probability using just laplace or add-k smoothing ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fe9760d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_log_prob_with_smoothing(test_data, unigrams, smoothing_method, vocab_size, k):\n",
    "    total_log_prob = 0\n",
    "\n",
    "    for word in test_data:\n",
    "        if smoothing_method == 'laplace':\n",
    "            prob = (unigrams.get(word, 0) + 1) / (sum(unigrams.values()) + vocab_size)\n",
    "        elif smoothing_method == 'add-k':\n",
    "            prob = (unigrams.get(word, 0) + k) / (sum(unigrams.values()) + k * vocab_size)\n",
    "            \n",
    "        total_log_prob += math.log(prob)\n",
    "\n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38af47",
   "metadata": {},
   "source": [
    "> Calculate Unigram total log probability using UNK token tagging where we add a UNK tag in unigrams from train dataset where the probabilities are recalculated using laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c6acc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_log_prob_unk_method2_with_laplace(test_data, unigrams, vocab_size):\n",
    "    total_log_prob = 0\n",
    "    total_words = sum(unigrams.values())\n",
    "    unigram_probs = defaultdict(float)\n",
    "    \n",
    "    for word in unigrams:\n",
    "        unigram_probs[word] = (unigrams.get(word, 0)+ 1) / (total_words + vocab_size)\n",
    "        \n",
    "    unigram_probs['<UNK>'] = 1 / (total_words + vocab_size)\n",
    "\n",
    "\n",
    "    for word in test_data:\n",
    "        if word not in unigrams:\n",
    "            word = '<UNK>'\n",
    "        total_log_prob += math.log(unigram_probs[word])\n",
    "    \n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8034c2f7",
   "metadata": {},
   "source": [
    "> Calculate Unigram total log probability using UNK token tagging where we add a UNK tag in unigrams from train dataset where the probabilities are recalculated using add-k smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f68a5bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_unigram_log_prob_unk_method2_add_k(test_data, unigrams, vocab_size, k):\n",
    "    total_log_prob = 0\n",
    "    total_words = sum(unigrams.values())\n",
    "    total_words = sum(unigrams.values())\n",
    "    unigram_probs = defaultdict(float)\n",
    "    \n",
    "    for word in unigrams:\n",
    "        unigram_probs[word] = (unigrams.get(word, 0) + k) / (total_words + k * vocab_size)\n",
    "        \n",
    "    unigram_probs['<UNK>'] = ( k )/(total_words +  k * vocab_size)\n",
    "\n",
    "\n",
    "    for word in test_data:\n",
    "        if word not in unigrams:\n",
    "            word = '<UNK>'\n",
    "        total_log_prob += math.log(unigram_probs[word])\n",
    "\n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ad684",
   "metadata": {},
   "source": [
    "> Calculate Bigram total log probability using UNK token tagging method by creating Implicit vocabulary with size v and choosing top v words by frequency and replacing rest with UNK. Here size is choosen as 3076 where the bottom tokens have occurence less than frequency of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1d1a1752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_log_prob_unk_method1(test_data, bigrams, unigrams):\n",
    "    total_log_prob = 0\n",
    "    N = len(test_data)\n",
    "    unigram_probs = defaultdict(float)\n",
    "    bigram_probs = defaultdict(float)\n",
    "    bigrams_new = defaultdict(float)\n",
    "    vocab_size_new = 3076\n",
    "    \n",
    "    unigrams_sorted = sorted(unigrams, key = unigrams.get, reverse = True)\n",
    "    \n",
    "    for i in range(3076):\n",
    "        word = unigrams_sorted[i]\n",
    "        unigram_probs[word] = (unigrams.get(word, 0)) + 1\n",
    "        \n",
    "    unigram_probs['<UNK>'] = (len(unigrams) - vocab_size_new)\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        prev_word,current_word = bigram\n",
    "        if prev_word not in unigram_probs:\n",
    "            prev_word = '<UNK>'\n",
    "        if current_word not in unigram_probs:\n",
    "            current_word = '<UNK>'\n",
    "        bigram = (prev_word, current_word)\n",
    "        \n",
    "        bigrams_new[bigram] = bigrams_new.get(bigram, 0) + 1\n",
    "\n",
    "        \n",
    "    for i in range(1, N):\n",
    "        prev_word = test_data[i - 1]\n",
    "        current_word = test_data[i]\n",
    "        if prev_word not in unigram_probs:\n",
    "            prev_word = '<UNK>'\n",
    "        if current_word not in unigram_probs:\n",
    "            current_word = '<UNK>'\n",
    "        \n",
    "        bigram = (prev_word, current_word)\n",
    "        \n",
    "        if bigram not in bigrams_new:\n",
    "            bigram = ('<UNK>', '<UNK>')\n",
    "        bigram_probs[bigram] = bigrams_new.get(bigram, 0) / unigram_probs[prev_word]\n",
    "        total_log_prob += math.log(bigram_probs[bigram])\n",
    "        \n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c712078",
   "metadata": {},
   "source": [
    "> Using the same above method calculated bigrams total log probability of test_data with laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8991c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_log_prob_unk_method1_laplace(test_data, bigrams, unigrams):\n",
    "    total_log_prob = 0\n",
    "    N = len(test_data)\n",
    "    unigram_probs = defaultdict(float)\n",
    "    bigram_probs = defaultdict(float)\n",
    "    bigrams_new = defaultdict(float)\n",
    "    vocab_size_new = 3076\n",
    "    \n",
    "    unigrams_sorted = sorted(unigrams, key = unigrams.get, reverse = True)\n",
    "    \n",
    "    for i in range(3076):\n",
    "        word = unigrams_sorted[i]\n",
    "        unigram_probs[word] = (unigrams.get(word, 0)) + 1\n",
    "\n",
    "        \n",
    "    unigram_probs['<UNK>'] = (len(unigrams) - vocab_size_new)\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        prev_word, current_word = bigram\n",
    "        if prev_word not in unigram_probs:\n",
    "            prev_word = '<UNK>'\n",
    "        if current_word not in unigram_probs:\n",
    "            current_word = '<UNK>'\n",
    "        bigram = (prev_word, current_word)\n",
    "        \n",
    "        bigrams_new[bigram] = bigrams_new.get(bigram, 0) + 1\n",
    "\n",
    "        \n",
    "    for i in range(1, N):\n",
    "        prev_word = test_data[i - 1]\n",
    "        current_word = test_data[i]\n",
    "        if prev_word not in unigram_probs:\n",
    "            prev_word = '<UNK>'\n",
    "        if current_word not in unigram_probs:\n",
    "            current_word = '<UNK>'\n",
    "            \n",
    "        bigram = (prev_word, current_word)\n",
    "        \n",
    "        if bigram not in bigrams_new:\n",
    "            bigram = ('<UNK>', '<UNK>')\n",
    "            \n",
    "        bigram_probs[bigram] = (bigrams_new.get(bigram, 0) + 1) / (unigram_probs[prev_word] + vocab_size_new)\n",
    "        total_log_prob += math.log(bigram_probs[bigram])\n",
    "\n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166307a",
   "metadata": {},
   "source": [
    "> Using the same above method calculated bigrams total log probability of test_data with add-k smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "990a754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_log_prob_unk_method1_add_k(test_data, bigrams, unigrams, k):\n",
    "    total_log_prob = 0\n",
    "    N = len(test_data)\n",
    "    unigram_probs = defaultdict(float)\n",
    "    bigram_probs = defaultdict(float)\n",
    "    bigrams_new = defaultdict(float)\n",
    "    vocab_size_new = 3076\n",
    "    \n",
    "    unigrams_sorted = sorted(unigrams, key = unigrams.get, reverse = True)\n",
    "    \n",
    "    for i in range(3076):\n",
    "        word = unigrams_sorted[i]\n",
    "        unigram_probs[word] = (unigrams.get(word, 0)) + 1\n",
    "\n",
    "        \n",
    "    unigram_probs['<UNK>'] = (len(unigrams) - vocab_size_new)\n",
    "\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        prev_word, current_word = bigram\n",
    "        if prev_word not in unigram_probs:\n",
    "            prev_word = '<UNK>'\n",
    "        if current_word not in unigram_probs:\n",
    "            current_word = '<UNK>'\n",
    "        bigram = (prev_word, current_word)\n",
    "        \n",
    "        bigrams_new[bigram] = bigrams_new.get(bigram,0) + 1\n",
    "\n",
    "        \n",
    "    for i in range(1, N):\n",
    "        prev_word = test_data[i - 1]\n",
    "        current_word = test_data[i]\n",
    "        if prev_word not in unigram_probs:\n",
    "            prev_word = '<UNK>'\n",
    "        if current_word not in unigram_probs:\n",
    "            current_word = '<UNK>'\n",
    "        \n",
    "        bigram = (prev_word, current_word)\n",
    "        \n",
    "        if bigram not in bigrams_new:\n",
    "            bigram = ('<UNK>', '<UNK>')\n",
    "        bigram_probs[bigram] = (bigrams_new.get(bigram) + k) / (unigram_probs[prev_word] + k * vocab_size_new)\n",
    "        total_log_prob += math.log(bigram_probs[bigram])\n",
    "\n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d5a53",
   "metadata": {},
   "source": [
    "> Calculate total log probabilities using just laplace and add-k smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b20682ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_log_prob_smoothing(test_data, unigrams, bigrams, vocab_size, smoothing_method, k):\n",
    "    total_log_prob = 0\n",
    "    N = len(test_data)\n",
    "\n",
    "    for i in range(1, N):\n",
    "        prev_word = test_data[i - 1]\n",
    "        current_word = test_data[i]\n",
    "\n",
    "        if smoothing_method == 'laplace':\n",
    "            prob = (bigrams.get((prev_word, current_word), 0) + 1) / (unigrams.get(prev_word, 0) + vocab_size)\n",
    "        elif smoothing_method == 'add-k':\n",
    "            prob = (bigrams.get((prev_word, current_word), 0) + k) / (unigrams.get(prev_word, 0) + k * vocab_size)\n",
    "\n",
    "        total_log_prob += math.log(prob)\n",
    "        \n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e930d22",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9cc158",
   "metadata": {},
   "source": [
    "> To calculate perplexity from total log probability and size of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c7aa7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(total_log_prob, N):\n",
    "    \n",
    "    return math.exp(-total_log_prob / N)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a349d6",
   "metadata": {},
   "source": [
    "> Calculating total log probabilites by invoking the already defined methods for unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6e44883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_unigram_unk_method1 = calculate_unigram_log_prob_unk_method1(test_data, unigrams)\n",
    "log_prob_unigram_unk_method1_laplace = calculate_unigram_log_prob_unk_method1_laplace(test_data, unigrams)\n",
    "log_prob_unigram_unk_method1_add_k = calculate_unigram_log_prob_unk_method1_add_k(test_data, unigrams, 0.5)\n",
    "log_prob_unigram_laplace_smoothing = calculate_unigram_log_prob_with_smoothing(test_data, unigrams, 'laplace', vocab_size, 1)\n",
    "log_prob_unigram_add_k_smoothing = calculate_unigram_log_prob_with_smoothing(test_data, unigrams, 'add-k', vocab_size, 0.5)\n",
    "log_prob_unigram_unk_method2_laplace = calculate_unigram_log_prob_unk_method2_with_laplace(test_data, unigrams, vocab_size)\n",
    "log_prob_unigram_unk_method2_add_k = calculate_unigram_log_prob_unk_method2_add_k(test_data, unigrams, vocab_size, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485422b",
   "metadata": {},
   "source": [
    "> Calculating perplexity by invoking the already defined perplexity method for unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a81569ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_unigram_unk_method1 = calculate_perplexity(log_prob_unigram_unk_method1, N)\n",
    "perplexity_unigram_unk_method1_laplace = calculate_perplexity(log_prob_unigram_unk_method1_laplace, N)\n",
    "perplexity_unigram_unk_method1_add_k = calculate_perplexity(log_prob_unigram_unk_method1_add_k, N)\n",
    "perplexity_unigram_laplace_smoothing = calculate_perplexity(log_prob_unigram_laplace_smoothing, N)\n",
    "perplexity_unigram_add_k_smoothing = calculate_perplexity(log_prob_unigram_add_k_smoothing, N)\n",
    "perplexity_unigram_unk_method2_laplace = calculate_perplexity(log_prob_unigram_unk_method2_laplace, N)\n",
    "perplexity_unigram_unk_method2_add_k = calculate_perplexity(log_prob_unigram_unk_method2_add_k, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "80611a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity with unk method 1: 331.65965712657663\n",
      "Unigram Perplexity with unk method 1 and laplace: 334.572345391625\n",
      "Unigram Perplexity with unk method 1 and add-k: 332.9410792379095\n",
      "Unigram Perplexity with laplace smoothing: 550.4715746437497\n",
      "Unigram Perplexity with add-k smoothing: 555.2862387883032\n",
      "Unigram Perplexity with unk-laplace method 2: 550.4715746437497\n",
      "Unigram Perplexity with unk-add-k method 2: 555.2862387883032\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unigram Perplexity with unk method 1: {perplexity_unigram_unk_method1}\")\n",
    "print(f\"Unigram Perplexity with unk method 1 and laplace: {perplexity_unigram_unk_method1_laplace}\")\n",
    "print(f\"Unigram Perplexity with unk method 1 and add-k: {perplexity_unigram_unk_method1_add_k}\")\n",
    "print(f\"Unigram Perplexity with laplace smoothing: {perplexity_unigram_laplace_smoothing}\")\n",
    "print(f\"Unigram Perplexity with add-k smoothing: {perplexity_unigram_add_k_smoothing}\")\n",
    "print(f\"Unigram Perplexity with unk-laplace method 2: {perplexity_unigram_unk_method2_laplace}\")\n",
    "print(f\"Unigram Perplexity with unk-add-k method 2: {perplexity_unigram_unk_method2_add_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4388ec3",
   "metadata": {},
   "source": [
    "> Calculating total log probabilites by invoking the already defined methods for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "efe1017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_bigram_unk_method1 = calculate_bigram_log_prob_unk_method1(test_data, bigrams, unigrams)\n",
    "log_prob_bigram_unk_method1_laplace = calculate_bigram_log_prob_unk_method1_laplace(test_data, bigrams, unigrams)\n",
    "log_prob_bigram_unk_method1_add_k = calculate_bigram_log_prob_unk_method1_add_k(test_data, bigrams, unigrams, 0.5)\n",
    "log_prob_bigram_laplace_smoothing = calculate_bigram_log_prob_smoothing(test_data, unigrams, bigrams, vocab_size, 'laplace', 1)\n",
    "log_prob_bigram_add_k_smoothing1 = calculate_bigram_log_prob_smoothing(test_data, unigrams, bigrams, vocab_size, 'add-k', 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81273c23",
   "metadata": {},
   "source": [
    "> Calculating perplexity by invoking the already defined perplexity method for unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ba0879cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_bigram_unk_method1 = calculate_perplexity(log_prob_bigram_unk_method1, N)\n",
    "perplexity_bigram_unk_method1_laplace = calculate_perplexity(log_prob_bigram_unk_method1_laplace, N)\n",
    "perplexity_bigram_unk_method1_add_k = calculate_perplexity(log_prob_bigram_unk_method1_add_k, N)\n",
    "perplexity_bigram_laplace_smoothing = calculate_perplexity(log_prob_bigram_laplace_smoothing, N)\n",
    "perplexity_bigram_add_k_smoothing1 = calculate_perplexity(log_prob_bigram_add_k_smoothing1, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c980969c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Perplexity with unk method 1: 44.64114699183954\n",
      "Bigram Perplexity with unk method 1 and laplace: 447.9366911925982\n",
      "Bigram Perplexity with unk method 1 and add-k: 316.6566028361528\n",
      "Bigram Perplexity with laplace smoothing: 1320.2702645843851\n",
      "Bigram Perplexity with add-k smoothing 1: 975.3721300180292\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bigram Perplexity with unk method 1: {perplexity_bigram_unk_method1}\")\n",
    "print(f\"Bigram Perplexity with unk method 1 and laplace: {perplexity_bigram_unk_method1_laplace}\")\n",
    "print(f\"Bigram Perplexity with unk method 1 and add-k: {perplexity_bigram_unk_method1_add_k}\")\n",
    "print(f\"Bigram Perplexity with laplace smoothing: {perplexity_bigram_laplace_smoothing}\")\n",
    "print(f\"Bigram Perplexity with add-k smoothing 1: {perplexity_bigram_add_k_smoothing1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e70962d",
   "metadata": {},
   "source": [
    "## Calculating perplexities on train data for uni and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "99d3eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_unigram_unk_method1_train = calculate_unigram_log_prob_unk_method1(train_data, unigrams)\n",
    "log_prob_unigram_unk_method1_laplace_train = calculate_unigram_log_prob_unk_method1_laplace(train_data, unigrams)\n",
    "log_prob_unigram_unk_method1_add_k_train = calculate_unigram_log_prob_unk_method1_add_k(train_data, unigrams, 0.5)\n",
    "log_prob_unigram_laplace_smoothing_train = calculate_unigram_log_prob_with_smoothing(train_data, unigrams, 'laplace', vocab_size, 1)\n",
    "log_prob_unigram_add_k_smoothing_train = calculate_unigram_log_prob_with_smoothing(train_data, unigrams, 'add-k', vocab_size, 0.5)\n",
    "log_prob_unigram_unk_method2_laplace_train = calculate_unigram_log_prob_unk_method2_with_laplace(train_data, unigrams, vocab_size)\n",
    "log_prob_unigram_unk_method2_add_k_train = calculate_unigram_log_prob_unk_method2_add_k(train_data, unigrams, vocab_size, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8092c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_unigram_unk_method1_train = calculate_perplexity(log_prob_unigram_unk_method1_train, n)\n",
    "perplexity_unigram_unk_method1_laplace_train = calculate_perplexity(log_prob_unigram_unk_method1_laplace_train, n)\n",
    "perplexity_unigram_unk_method1_add_k_train = calculate_perplexity(log_prob_unigram_unk_method1_add_k_train, n)\n",
    "perplexity_unigram_laplace_smoothing_train = calculate_perplexity(log_prob_unigram_laplace_smoothing_train, n)\n",
    "perplexity_unigram_add_k_smoothing_train = calculate_perplexity(log_prob_unigram_add_k_smoothing_train, n)\n",
    "perplexity_unigram_unk_method2_laplace_train = calculate_perplexity(log_prob_unigram_unk_method2_laplace_train, n)\n",
    "perplexity_unigram_unk_method2_add_k_train = calculate_perplexity(log_prob_unigram_unk_method2_add_k_train, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f229f859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity with unk method 1 train: 381.20140975076384\n",
      "Unigram Perplexity with unk method 1 and laplace train: 382.4474713719824\n",
      "Unigram Perplexity with unk method 1 and add-k train: 381.5502932620644\n",
      "Unigram Perplexity with laplace smoothing train: 529.447063921777\n",
      "Unigram Perplexity with add-k smoothing train: 524.6629051608547\n",
      "Unigram Perplexity with unk-laplace train method 2: 529.447063921777\n",
      "Unigram Perplexity with unk-add-k train method 2: 524.6629051608547\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unigram Perplexity with unk method 1 train: {perplexity_unigram_unk_method1_train}\")\n",
    "print(f\"Unigram Perplexity with unk method 1 and laplace train: {perplexity_unigram_unk_method1_laplace_train}\")\n",
    "print(f\"Unigram Perplexity with unk method 1 and add-k train: {perplexity_unigram_unk_method1_add_k_train}\")\n",
    "print(f\"Unigram Perplexity with laplace smoothing train: {perplexity_unigram_laplace_smoothing_train}\")\n",
    "print(f\"Unigram Perplexity with add-k smoothing train: {perplexity_unigram_add_k_smoothing_train}\")\n",
    "print(f\"Unigram Perplexity with unk-laplace train method 2: {perplexity_unigram_unk_method2_laplace_train}\")\n",
    "print(f\"Unigram Perplexity with unk-add-k train method 2: {perplexity_unigram_unk_method2_add_k_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "06385eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_bigram_unk_method1_train = calculate_bigram_log_prob_unk_method1(train_data, bigrams, unigrams)\n",
    "log_prob_bigram_unk_method1_laplace_train = calculate_bigram_log_prob_unk_method1_laplace(train_data, bigrams, unigrams)\n",
    "log_prob_bigram_unk_method1_add_k_train = calculate_bigram_log_prob_unk_method1_add_k(train_data, bigrams, unigrams, 0.5)\n",
    "log_prob_bigram_laplace_smoothing_train = calculate_bigram_log_prob_smoothing(train_data, unigrams, bigrams, vocab_size, 'laplace', 1)\n",
    "log_prob_bigram_add_k_smoothing_train1 = calculate_bigram_log_prob_smoothing(train_data, unigrams, bigrams, vocab_size, 'add-k', 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "11999c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_bigram_unk_method1_train = calculate_perplexity(log_prob_bigram_unk_method1_train, n)\n",
    "perplexity_bigram_unk_method1_laplace_train = calculate_perplexity(log_prob_bigram_unk_method1_laplace_train, n)\n",
    "perplexity_bigram_unk_method1_add_k_train = calculate_perplexity(log_prob_bigram_unk_method1_add_k_train, n)\n",
    "perplexity_bigram_laplace_smoothing_train = calculate_perplexity(log_prob_bigram_laplace_smoothing_train, n)\n",
    "perplexity_bigram_add_k_smoothing1_train = calculate_perplexity(log_prob_bigram_add_k_smoothing_train1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "67da102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Perplexity with unk method 1 train: 174.3369949437968\n",
      "Bigram Perplexity with unk method 1 and laplace train: 1621.2261744384784\n",
      "Bigram Perplexity with unk method 1 and add-k train: 1233.1386552876177\n",
      "Bigram Perplexity with laplace smoothing train: 981.1156826539259\n",
      "Bigram Perplexity with add-k smoothing 1 train: 620.2636955844678\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bigram Perplexity with unk method 1 train: {perplexity_bigram_unk_method1_train}\")\n",
    "print(f\"Bigram Perplexity with unk method 1 and laplace train: {perplexity_bigram_unk_method1_laplace_train}\")\n",
    "print(f\"Bigram Perplexity with unk method 1 and add-k train: {perplexity_bigram_unk_method1_add_k_train}\")\n",
    "print(f\"Bigram Perplexity with laplace smoothing train: {perplexity_bigram_laplace_smoothing_train}\")\n",
    "print(f\"Bigram Perplexity with add-k smoothing 1 train: {perplexity_bigram_add_k_smoothing1_train}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
